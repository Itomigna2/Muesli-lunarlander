{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3w04_bU_4H9g"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gnwrapper\n",
    "\n",
    "!nvidia-smi\n",
    "print(torch.cuda.is_available())\n",
    "print(\"library installation is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgMBnokKfDY7"
   },
   "outputs": [],
   "source": [
    "## For linear lr decay\n",
    "## https://github.com/cmpark0126/pytorch-polynomial-lr-decay\n",
    "! pip install git+https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git\n",
    "from torch_poly_lr_decay import PolynomialLRDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_n1jjuq_TCF"
   },
   "outputs": [],
   "source": [
    "class Representation(nn.Module): \n",
    "    \"\"\"Representation Network\n",
    "\n",
    "    Representation network produces hidden state from observations.\n",
    "    Hidden state scaled within the bounds of [-1,1]. \n",
    "    Simple mlp network used with 1 skip connection.\n",
    "\n",
    "    input : raw input\n",
    "    output : hs(hidden state) \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, width):\n",
    "        super().__init__()\n",
    "        self.skip = torch.nn.Linear(input_dim, output_dim)  \n",
    "        self.layer1 = torch.nn.Linear(input_dim, width)\n",
    "        self.layer2 = torch.nn.Linear(width, width)\n",
    "        self.layer3 = torch.nn.Linear(width, width) \n",
    "        self.layer4 = torch.nn.Linear(width, width)  \n",
    "        self.layer5 = torch.nn.Linear(width, output_dim)     \n",
    "        \n",
    "    def forward(self, x):\n",
    "        s = self.skip(x)\n",
    "        x = self.layer1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.layer4(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.layer5(x)    \n",
    "        x = torch.nn.functional.relu(x+s)\n",
    "        x = 2*(x - x.min(-1,keepdim=True)[0])/(x.max(-1,keepdim=True)[0] - x.min(-1,keepdim=True)[0])-1 \n",
    "        return x\n",
    "\n",
    "\n",
    "class Dynamics(nn.Module): \n",
    "    \"\"\"Dynamics Network\n",
    "\n",
    "    Dynamics network transits (hidden state + action) to next hidden state and inferences reward model.\n",
    "    Hidden state scaled within the bounds of [-1,1]. Action encoded to one-hot representation. \n",
    "    Zeros tensor is used for action -1.\n",
    "    \n",
    "    Output of the reward head is categorical representation, instaed of scalar value.\n",
    "    Categorical output will be converted to scalar value with 'to_scalar()',and when \n",
    "    traning target value will be converted to categorical target with 'to_cr()'.\n",
    "    \n",
    "    input : hs, action\n",
    "    output : next_hs, reward \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, width, action_space):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_dim + action_space, width)\n",
    "        self.layer2 = torch.nn.Linear(width, width) \n",
    "        self.hs_head = torch.nn.Linear(width, output_dim)\n",
    "        self.reward_head = nn.Sequential(\n",
    "            nn.Linear(width,width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(width,width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(width,support_size*2+1)           \n",
    "        ) \n",
    "        self.one_hot_act = torch.cat((torch.nn.functional.one_hot(torch.arange(0, action_space) % action_space, num_classes=action_space),\n",
    "                                      torch.zeros(action_space).unsqueeze(0)),\n",
    "                                      dim=0).to(device)        \n",
    "       \n",
    "    def forward(self, x, action):\n",
    "        action = self.one_hot_act[action.squeeze(1)]\n",
    "        x = torch.cat((x,action.to(device)), dim=1)\n",
    "        x = self.layer1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        hs = self.hs_head(x)\n",
    "        hs = torch.nn.functional.relu(hs)\n",
    "        reward = self.reward_head(x)    \n",
    "        hs = 2*(hs - hs.min(-1,keepdim=True)[0])/(hs.max(-1,keepdim=True)[0] - hs.min(-1,keepdim=True)[0])-1\n",
    "        return hs, reward\n",
    "\n",
    "\n",
    "class Prediction(nn.Module): \n",
    "    \"\"\"Prediction Network\n",
    "\n",
    "    Prediction network inferences probability distribution of policy and value model from hidden state. \n",
    "\n",
    "    Output of the value head is categorical representation, instaed of scalar value.\n",
    "    Categorical output will be converted to scalar value with 'to_scalar()',and when \n",
    "    traning target value will be converted to categorical target with 'to_cr()'.\n",
    "        \n",
    "    input : hs\n",
    "    output : P, V \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, width):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_dim, width)\n",
    "        self.layer2 = torch.nn.Linear(width, width) \n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(width,width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(width,width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(width,output_dim)           \n",
    "        ) \n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(width,width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(width,width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(width,support_size*2+1)           \n",
    "        ) \n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        P = self.policy_head(x)\n",
    "        P = torch.nn.functional.softmax(P, dim=-1) \n",
    "        V = self.value_head(x)      \n",
    "        return P, V\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For categorical representation\n",
    "reference : https://github.com/werner-duvaud/muzero-general\n",
    "In my opinion, support size have to cover the range of maximum absolute value of \n",
    "reward and value of entire trajectories. Support_size 30 can cover almost [-900,900].\n",
    "\"\"\"\n",
    "support_size = 30\n",
    "eps = 0.001\n",
    "\n",
    "def to_scalar(x):\n",
    "    x = torch.softmax(x, dim=-1)\n",
    "    probabilities = x\n",
    "    support = (torch.tensor([x for x in range(-support_size, support_size + 1)]).expand(probabilities.shape).float().to(device))\n",
    "    x = torch.sum(support * probabilities, dim=1, keepdim=True)\n",
    "    scalar = torch.sign(x) * (((torch.sqrt(1 + 4 * eps * (torch.abs(x) + 1 + eps)) - 1) / (2 * eps))** 2 - 1)\n",
    "    return scalar\n",
    "\n",
    "def to_scalar_no_soft(x): ## test purpose\n",
    "    probabilities = x \n",
    "    support = (torch.tensor([x for x in range(-support_size, support_size + 1)]).expand(probabilities.shape).float().to(device))\n",
    "    x = torch.sum(support * probabilities, dim=1, keepdim=True)\n",
    "    scalar = torch.sign(x) * (((torch.sqrt(1 + 4 * eps * (torch.abs(x) + 1 + eps)) - 1) / (2 * eps))** 2 - 1)\n",
    "    return scalar\n",
    "\n",
    "def to_cr(x):\n",
    "    x = x.squeeze(-1).unsqueeze(0)\n",
    "    x = torch.sign(x) * (torch.sqrt(torch.abs(x) + 1) - 1) + eps * x\n",
    "    x = torch.clip(x, -support_size, support_size)\n",
    "    floor = x.floor()\n",
    "    under = x - floor\n",
    "    floor_prob = (1 - under)\n",
    "    under_prob = under\n",
    "    floor_index = floor + support_size\n",
    "    under_index = floor + support_size + 1\n",
    "    logits = torch.zeros(x.shape[0], x.shape[1], 2 * support_size + 1).type(torch.float32).to(device)\n",
    "    logits.scatter_(2, floor_index.long().unsqueeze(-1), floor_prob.unsqueeze(-1))\n",
    "    under_prob = under_prob.masked_fill_(2 * support_size < under_index, 0.0)\n",
    "    under_index = under_index.masked_fill_(2 * support_size < under_index, 0.0)\n",
    "    logits.scatter_(2, under_index.long().unsqueeze(-1), under_prob.unsqueeze(-1))\n",
    "    return logits.squeeze(0)\n",
    "\n",
    "\n",
    "##Target network\n",
    "class Target(nn.Module):\n",
    "    \"\"\"Target Network\n",
    "    \n",
    "    Target network is used to approximate v_pi_prior, q_pi_prior, pi_prior.\n",
    "    It contains older network parameters. (exponential moving average update)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, width):\n",
    "        super().__init__()\n",
    "        self.representation_network = Representation(state_dim*8, state_dim*4, width) \n",
    "        self.dynamics_network = Dynamics(state_dim*4, state_dim*4, width, action_dim)\n",
    "        self.prediction_network = Prediction(state_dim*4, action_dim, width) \n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "##Muesli agent\n",
    "class Agent(nn.Module):\n",
    "    \"\"\"Agent Class\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, width):\n",
    "        super().__init__()\n",
    "        self.representation_network = Representation(state_dim*8, state_dim*4, width) \n",
    "        self.dynamics_network = Dynamics(state_dim*4, state_dim*4, width, action_dim)\n",
    "        self.prediction_network = Prediction(state_dim*4, action_dim, width) \n",
    "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=0.0003, weight_decay=0)\n",
    "        self.scheduler = PolynomialLRDecay(self.optimizer, max_decay_steps=4000, end_learning_rate=0.0000)   \n",
    "        self.to(device)\n",
    "\n",
    "        self.state_replay = []\n",
    "        self.action_replay = []\n",
    "        self.P_replay = []\n",
    "        self.r_replay = []   \n",
    "\n",
    "        self.action_space = action_dim\n",
    "        self.env = gym.make(game_name)     \n",
    "\n",
    "        self.var = 0\n",
    "        self.beta_product = 1.0\n",
    "\n",
    "        self.var_m = [0 for _ in range(5)]\n",
    "        self.beta_product_m = [1.0 for _ in range(5)] \n",
    "\n",
    "\n",
    "    def self_play_mu(self, target, max_timestep=1000):       \n",
    "        \"\"\"Self-play and save trajectory to replay buffer\n",
    "\n",
    "        Self-play with target network parameter\n",
    "\n",
    "        Eight previous observations stacked -> representation network -> prediction network \n",
    "        -> sampling action follow policy -> next env step\n",
    "        \"\"\"      \n",
    "\n",
    "        self.state_traj = []\n",
    "        self.action_traj = []\n",
    "        self.P_traj = []\n",
    "        self.r_traj = []      \n",
    "\n",
    "        game_score = 0\n",
    "        state = self.env.reset()\n",
    "        state = state[0]\n",
    "        state_dim = len(state)\n",
    "        for i in range(max_timestep):   \n",
    "            start_state = state\n",
    "            if i == 0:\n",
    "                stacked_state = np.concatenate((state, state, state, state, state, state, state, state), axis=0)\n",
    "            else:\n",
    "                stacked_state = np.roll(stacked_state,-state_dim,axis=0)                \n",
    "                stacked_state[-state_dim:]=state\n",
    "\n",
    "            with torch.no_grad():\n",
    "                hs = target.representation_network(torch.from_numpy(stacked_state).float().to(device))\n",
    "                P, v = target.prediction_network(hs)    \n",
    "            action = np.random.choice(np.arange(self.action_space), p=P.detach().cpu().numpy())   \n",
    "            state, r, done, info, _ = self.env.step(action)                    \n",
    "            \n",
    "            if i == 0:\n",
    "                for _ in range(8):\n",
    "                    self.state_traj.append(start_state)\n",
    "            else:\n",
    "                self.state_traj.append(start_state)\n",
    "            self.action_traj.append(action)\n",
    "            self.P_traj.append(P.cpu().numpy())\n",
    "            self.r_traj.append(r)\n",
    "            \n",
    "            game_score += r\n",
    "\n",
    "            if done:\n",
    "                last_frame = i\n",
    "                break\n",
    "\n",
    "        #print('self_play: score, r, done, info, lastframe', int(game_score), r, done, info, i)\n",
    "\n",
    "\n",
    "        # for update inference over trajectory length\n",
    "        for _ in range(5):\n",
    "            self.state_traj.append(np.zeros_like(state))\n",
    "\n",
    "        for _ in range(6):\n",
    "            self.r_traj.append(0.0)\n",
    "            self.action_traj.append(-1)  \n",
    "\n",
    "        # traj append to replay\n",
    "        self.state_replay.append(self.state_traj)\n",
    "        self.action_replay.append(self.action_traj)\n",
    "        self.P_replay.append(self.P_traj)\n",
    "        self.r_replay.append(self.r_traj)  \n",
    "\n",
    "        writer.add_scalars('Selfplay',\n",
    "                           {'lastreward': r,\n",
    "                           # 'lastframe': last_frame+1\n",
    "                           },global_i)\n",
    "\n",
    "        return game_score , r, last_frame\n",
    "\n",
    "\n",
    "    def update_weights_mu(self, target):\n",
    "        \"\"\"Optimize network weights.\n",
    "\n",
    "        Iteration: 80\n",
    "        Mini-batch size: 16 (1 seqeuence in 1 replay)\n",
    "        Replay: 25% online data\n",
    "        Discount: 0.997\n",
    "        Unroll: 5 step\n",
    "        L_m: 5 step(Muesli)\n",
    "        Observations: Stack 8 frame\n",
    "        regularizer_multiplier: 5 \n",
    "        Loss: L_pg_cmpo + L_v/6/4 + L_r/5/1 + L_m\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(80): \n",
    "            state_traj = []\n",
    "            action_traj = []\n",
    "            P_traj = []\n",
    "            r_traj = []      \n",
    "            G_arr_mb = []\n",
    "\n",
    "            for epi_sel in range(16):\n",
    "                if(epi_sel>3):## replay proportion\n",
    "                    sel = np.random.randint(0,len(self.state_replay)) \n",
    "                else:\n",
    "                    sel = -1\n",
    "\n",
    "                ## multi step return G (orignally retrace used)\n",
    "                G = 0\n",
    "                G_arr = []\n",
    "                for r in self.r_replay[sel][::-1]:\n",
    "                    G = 0.997 * G + r\n",
    "                    G_arr.append(G)\n",
    "                G_arr.reverse()\n",
    "                \n",
    "                for i in np.random.randint(len(self.state_replay[sel])-5-7,size=1):\n",
    "                    state_traj.append(self.state_replay[sel][i:i+13])\n",
    "                    action_traj.append(self.action_replay[sel][i:i+5])\n",
    "                    r_traj.append(self.r_replay[sel][i:i+5])\n",
    "                    G_arr_mb.append(G_arr[i:i+6])                        \n",
    "                    P_traj.append(self.P_replay[sel][i])\n",
    "\n",
    "\n",
    "            state_traj = torch.from_numpy(np.array(state_traj)).to(device)\n",
    "            action_traj = torch.from_numpy(np.array(action_traj)).unsqueeze(2).to(device)\n",
    "            P_traj = torch.from_numpy(np.array(P_traj)).to(device)\n",
    "            G_arr_mb = torch.from_numpy(np.array(G_arr_mb)).unsqueeze(2).float().to(device)\n",
    "            r_traj = torch.from_numpy(np.array(r_traj)).unsqueeze(2).float().to(device)\n",
    "            inferenced_P_arr = []\n",
    "\n",
    "            ## stacking 8 frame\n",
    "            stacked_state_0 = torch.cat((state_traj[:,0], state_traj[:,1], state_traj[:,2], state_traj[:,3],\n",
    "                                         state_traj[:,4], state_traj[:,5], state_traj[:,6], state_traj[:,7]), dim=1)\n",
    "\n",
    "\n",
    "            ## agent network inference (5 step unroll)\n",
    "            first_hs = self.representation_network(stacked_state_0)\n",
    "            first_P, first_v_logits = self.prediction_network(first_hs)      \n",
    "            inferenced_P_arr.append(first_P)\n",
    "\n",
    "            second_hs, r_logits = self.dynamics_network(first_hs, action_traj[:,0])    \n",
    "            second_P, second_v_logits = self.prediction_network(second_hs)\n",
    "            inferenced_P_arr.append(second_P)\n",
    "\n",
    "            third_hs, r2_logits = self.dynamics_network(second_hs, action_traj[:,1])    \n",
    "            third_P, third_v_logits = self.prediction_network(third_hs)\n",
    "            inferenced_P_arr.append(third_P)\n",
    "\n",
    "            fourth_hs, r3_logits = self.dynamics_network(third_hs, action_traj[:,2])    \n",
    "            fourth_P, fourth_v_logits = self.prediction_network(fourth_hs)\n",
    "            inferenced_P_arr.append(fourth_P)\n",
    "\n",
    "            fifth_hs, r4_logits = self.dynamics_network(fourth_hs, action_traj[:,3])    \n",
    "            fifth_P, fifth_v_logits = self.prediction_network(fifth_hs)\n",
    "            inferenced_P_arr.append(fifth_P)\n",
    "\n",
    "            sixth_hs, r5_logits = self.dynamics_network(fifth_hs, action_traj[:,4])    \n",
    "            sixth_P, sixth_v_logits = self.prediction_network(sixth_hs)\n",
    "            inferenced_P_arr.append(sixth_P)\n",
    "\n",
    "\n",
    "            ## target network inference\n",
    "            with torch.no_grad():\n",
    "                t_first_hs = target.representation_network(stacked_state_0)\n",
    "                t_first_P, t_first_v_logits = target.prediction_network(t_first_hs)  \n",
    "\n",
    "\n",
    "            ## normalized advantage\n",
    "            beta_var = 0.99\n",
    "            self.var = beta_var*self.var + (1-beta_var)*(torch.sum((G_arr_mb[:,0] - to_scalar(t_first_v_logits))**2)/16)\n",
    "            self.beta_product *= beta_var\n",
    "            var_hat = self.var/(1-self.beta_product)\n",
    "            under = torch.sqrt(var_hat + 1e-12)\n",
    "\n",
    "\n",
    "            ## L_pg_cmpo first term (eq.10)\n",
    "            importance_weight = torch.clip(first_P.gather(1,action_traj[:,0])\n",
    "                                        /(P_traj.gather(1,action_traj[:,0])),\n",
    "                                        0, 1\n",
    "            )\n",
    "            first_term = -1 * importance_weight * (G_arr_mb[:,0] - to_scalar(t_first_v_logits))/under\n",
    "\n",
    "\n",
    "            ##lookahead inferences (one step look-ahead to some actions to estimate q_prior, from target network)\n",
    "            with torch.no_grad():                                \n",
    "                r1_arr = []\n",
    "                v1_arr = []\n",
    "                a1_arr = []\n",
    "                for _ in range(self.action_space): #sample <= N(action space), now N    \n",
    "                    action1_stack = []\n",
    "                    for p in t_first_P:             \n",
    "                        action1_stack.append(np.random.choice(np.arange(self.action_space), p=p.detach().cpu().numpy()))    \n",
    "                    hs, r1 = target.dynamics_network(t_first_hs, torch.unsqueeze(torch.tensor(action1_stack),1))\n",
    "                    _, v1 = target.prediction_network(hs)\n",
    "\n",
    "                    r1_arr.append(to_scalar(r1))\n",
    "                    v1_arr.append(to_scalar(v1))\n",
    "                    a1_arr.append(torch.tensor(action1_stack))               \n",
    "\n",
    "            ## z_cmpo_arr (eq.12)\n",
    "            with torch.no_grad():   \n",
    "                exp_clip_adv_arr = [torch.exp(torch.clip((r1_arr[k] + 0.997 * v1_arr[k] - to_scalar(t_first_v_logits))/under, -1, 1))\n",
    "                                    .tolist() for k in range(self.action_space)]\n",
    "                exp_clip_adv_arr = torch.tensor(exp_clip_adv_arr).to(device)\n",
    "                z_cmpo_arr = []\n",
    "                for k in range(self.action_space):\n",
    "                    z_cmpo = (1 + torch.sum(exp_clip_adv_arr[k],dim=0) - exp_clip_adv_arr[k]) / self.action_space \n",
    "                    z_cmpo_arr.append(z_cmpo.tolist())\n",
    "            z_cmpo_arr = torch.tensor(z_cmpo_arr).to(device)\n",
    "\n",
    "\n",
    "            ## L_pg_cmpo second term (eq.11)\n",
    "            second_term = 0\n",
    "            for k in range(self.action_space):\n",
    "                second_term += exp_clip_adv_arr[k]/z_cmpo_arr[k] * torch.log(first_P.gather(1, torch.unsqueeze(a1_arr[k],1).to(device)))\n",
    "            regularizer_multiplier = 5\n",
    "            second_term *= -1 * regularizer_multiplier / self.action_space\n",
    "\n",
    "\n",
    "            ## L_pg_cmpo               \n",
    "            L_pg_cmpo = first_term + second_term\n",
    "\n",
    "\n",
    "            ## L_m\n",
    "            L_m  = 0\n",
    "            for i in range(5):\n",
    "                stacked_state = torch.cat(( state_traj[:,i+1], state_traj[:,i+2], state_traj[:,i+3], state_traj[:,i+4],\n",
    "                                            state_traj[:,i+5], state_traj[:,i+6], state_traj[:,i+7], state_traj[:,i+8]), dim=1)\n",
    "                with torch.no_grad():\n",
    "                    t_hs = target.representation_network(stacked_state)\n",
    "                    t_P, t_v_logits = target.prediction_network(t_hs) \n",
    "\n",
    "                beta_var = 0.99\n",
    "                self.var_m[i] = beta_var*self.var_m[i] + (1-beta_var)*(torch.sum((G_arr_mb[:,i+1] - to_scalar(t_v_logits))**2)/16)\n",
    "                self.beta_product_m[i]  *= beta_var\n",
    "                var_hat = self.var_m[i] /(1-self.beta_product_m[i])\n",
    "                under = torch.sqrt(var_hat + 1e-12)\n",
    "\n",
    "                with torch.no_grad():                                \n",
    "                    r1_arr = []\n",
    "                    v1_arr = []\n",
    "                    a1_arr = []\n",
    "                    for j in range(self.action_space):  \n",
    "                        action1_stack = []\n",
    "                        for _ in t_P:             \n",
    "                            action1_stack.append(j)    \n",
    "                        hs, r1 = target.dynamics_network(t_hs, torch.unsqueeze(torch.tensor(action1_stack),1))\n",
    "                        _, v1 = target.prediction_network(hs)\n",
    "\n",
    "                        r1_arr.append(to_scalar(r1))\n",
    "                        v1_arr.append(to_scalar(v1))\n",
    "                        a1_arr.append(torch.tensor(action1_stack))    \n",
    "\n",
    "                with torch.no_grad():   \n",
    "                    exp_clip_adv_arr = [torch.exp(torch.clip((r1_arr[k] + 0.997 * v1_arr[k] - to_scalar(t_v_logits))/under,-1, 1))\n",
    "                                        .tolist() for k in range(self.action_space)]\n",
    "                    exp_clip_adv_arr = torch.tensor(exp_clip_adv_arr).to(device)\n",
    "\n",
    "                ## Paper appendix F.2 : Prior policy\n",
    "                t_P = 0.967*t_P + 0.03*P_traj + 0.003*torch.tensor([[0.25,0.25,0.25,0.25] for _ in range(16)]).to(device) \n",
    "\n",
    "                pi_cmpo_all = [(t_P.gather(1, torch.unsqueeze(a1_arr[k],1).to(device)) \n",
    "                                * exp_clip_adv_arr[k])\n",
    "                                .squeeze(-1).tolist() for k in range(self.action_space)]                \n",
    "        \n",
    "                pi_cmpo_all = torch.tensor(pi_cmpo_all).transpose(0,1).to(device)\n",
    "                pi_cmpo_all = pi_cmpo_all/torch.sum(pi_cmpo_all,dim=1).unsqueeze(-1)\n",
    "                kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "                L_m += kl_loss(torch.log(inferenced_P_arr[i+1]), pi_cmpo_all) \n",
    "            \n",
    "            L_m/=5\n",
    "\n",
    "\n",
    "            ## L_v\n",
    "            ls = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "            L_v = -1 * (\n",
    "                (to_cr(G_arr_mb[:,0])*ls(first_v_logits)).sum(-1, keepdim=True)\n",
    "                +  (to_cr(G_arr_mb[:,1])*ls(second_v_logits)).sum(-1, keepdim=True)\n",
    "                +  (to_cr(G_arr_mb[:,2])*ls(third_v_logits)).sum(-1, keepdim=True)\n",
    "                +  (to_cr(G_arr_mb[:,3])*ls(fourth_v_logits)).sum(-1, keepdim=True)\n",
    "                +  (to_cr(G_arr_mb[:,4])*ls(fifth_v_logits)).sum(-1, keepdim=True)\n",
    "                +  (to_cr(G_arr_mb[:,5])*ls(sixth_v_logits)).sum(-1, keepdim=True)\n",
    "            )\n",
    "\n",
    "            ## L_r     \n",
    "            L_r = -1 * (\n",
    "                (to_cr(r_traj[:,0])*ls(r_logits)).sum(-1, keepdim=True)\n",
    "                + (to_cr(r_traj[:,1])*ls(r2_logits)).sum(-1, keepdim=True)\n",
    "                + (to_cr(r_traj[:,2])*ls(r3_logits)).sum(-1, keepdim=True)\n",
    "                + (to_cr(r_traj[:,3])*ls(r4_logits)).sum(-1, keepdim=True)\n",
    "                + (to_cr(r_traj[:,4])*ls(r5_logits)).sum(-1, keepdim=True)\n",
    "            )\n",
    "\n",
    "\n",
    "            ## start of dynamics network gradient *0.5\n",
    "            first_hs.register_hook(lambda grad: grad * 0.5)\n",
    "            second_hs.register_hook(lambda grad: grad * 0.5) \n",
    "            third_hs.register_hook(lambda grad: grad * 0.5)    \n",
    "            fourth_hs.register_hook(lambda grad: grad * 0.5)   \n",
    "            fifth_hs.register_hook(lambda grad: grad * 0.5)  \n",
    "            sixth_hs.register_hook(lambda grad: grad * 0.5)  \n",
    "\n",
    "\n",
    "            ## total loss\n",
    "            L_total = L_pg_cmpo + L_v/6/4 + L_r/5/1 + L_m   \n",
    "          \n",
    "            \n",
    "            ## optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            L_total.mean().backward()\n",
    "            nn.utils.clip_grad_value_(self.parameters(), clip_value=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "\n",
    "            ## target network(prior parameters) moving average update\n",
    "            alpha_target = 0.01 \n",
    "            params1 = self.named_parameters()\n",
    "            params2 = target.named_parameters()\n",
    "            dict_params2 = dict(params2)\n",
    "            for name1, param1 in params1:\n",
    "                if name1 in dict_params2:\n",
    "                    dict_params2[name1].data.copy_(alpha_target*param1.data + (1-alpha_target)*dict_params2[name1].data)\n",
    "            target.load_state_dict(dict_params2)          \n",
    "\n",
    "\n",
    "        self.scheduler.step()\n",
    "\n",
    "\n",
    "        writer.add_scalars('Loss',{'L_total': L_total.mean(),\n",
    "                                  'L_pg_cmpo': L_pg_cmpo.mean(),\n",
    "                                  'L_v': (L_v/6/4).mean(),\n",
    "                                  'L_r': (L_r/5/1).mean(),\n",
    "                                  'L_m': (L_m).mean()\n",
    "                                  },global_i)\n",
    "        \n",
    "        writer.add_scalars('vars',{'self.var':self.var,\n",
    "                                   'self.var_m':self.var_m[0]\n",
    "                                  },global_i)\n",
    "        \n",
    "        return\n",
    "\n",
    "#%rm -rf scalar/\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir scalar --port=6010\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "score_arr = []\n",
    "game_name = 'LunarLander-v2' \n",
    "env = gym.make(game_name) \n",
    "target = Target(env.observation_space.shape[0], env.action_space.n, 128)\n",
    "agent = Agent(env.observation_space.shape[0], env.action_space.n, 128)  \n",
    "print(agent)\n",
    "env.close()\n",
    "\n",
    "## initialization\n",
    "target.load_state_dict(agent.state_dict())\n",
    "\n",
    "## Self play & Weight update loop\n",
    "episode_nums = 4000\n",
    "for i in range(episode_nums):\n",
    "    writer = SummaryWriter(logdir='scalar/')\n",
    "    global_i = i    \n",
    "    game_score , last_r, frame = agent.self_play_mu(target)       \n",
    "    writer.add_scalar('score', game_score, global_i)    \n",
    "    score_arr.append(game_score)  \n",
    "    print('episode, score, last_r, len\\n', i, int(game_score), last_r, frame)\n",
    "\n",
    "    if i%100==0:\n",
    "        torch.save(target.state_dict(), 'weights_target.pt') \n",
    "\n",
    "    if game_score > 250 and np.mean(np.array(score_arr[-20:])) > 250:\n",
    "        torch.save(target.state_dict(), 'weights_target.pt') \n",
    "        print('Done')\n",
    "        break\n",
    "\n",
    "    agent.update_weights_mu(target) \n",
    "    writer.close()\n",
    "\n",
    "torch.save(target.state_dict(), 'weights_target.pt')  \n",
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": {}
    },
    "id": "F2MbdWmPFoEK",
    "outputId": "7b1c97d7-3231-4ad3-b3d3-b322fd8ec49a"
   },
   "outputs": [],
   "source": [
    "## Earned score per episode\n",
    "\n",
    "window = 30\n",
    "mean_arr = []\n",
    "for i in range(len(score_arr) - window + 1):\n",
    "    mean_arr.append(np.mean(np.array(score_arr[i:i+window])))\n",
    "for i in range(window - 1):\n",
    "    mean_arr.insert(0, np.nan)\n",
    "\n",
    "plt.plot(score_arr, label ='score')\n",
    "plt.plot(mean_arr, label ='mean')\n",
    "plt.ylim([-300,300])\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpeJyIbOeY2q"
   },
   "outputs": [],
   "source": [
    "## game play video(target network)\n",
    "target.load_state_dict(torch.load(\"weights_target.pt\"))\n",
    "env = gnwrapper.LoopAnimation(gym.make(game_name)) \n",
    "state = env.reset()\n",
    "state_dim = len(state)\n",
    "game_score = 0\n",
    "score_arr2 = []\n",
    "state_arr = []\n",
    "for i in range(1000):\n",
    "    if i == 0:\n",
    "        stacked_state = np.concatenate((state, state, state, state, state, state, state, state), axis=0)\n",
    "    else:\n",
    "        stacked_state = np.roll(stacked_state,-state_dim,axis=0)        \n",
    "        stacked_state[-state_dim:]=state\n",
    "    with torch.no_grad():\n",
    "        hs = target.representation_network(torch.from_numpy(stacked_state).float().to(device))\n",
    "        P, v = target.prediction_network(hs)\n",
    "        action = np.random.choice(np.arange(agent.action_space), p=P.detach().numpy())   \n",
    "    if i %5==0:\n",
    "        env.render()\n",
    "    state, r, done, info = env.step(action.item())\n",
    "    print(r, done, info)\n",
    "    state_arr.append(state[0])\n",
    "    game_score += r \n",
    "    score_arr2.append(game_score)\n",
    "    if i %10==0: \n",
    "        print(game_score)\n",
    "    if done:\n",
    "        print('last frame number : ',i)\n",
    "        print('score :', game_score)\n",
    "        print(state_arr)\n",
    "       \n",
    "        break\n",
    "env.reset()\n",
    "env.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArdvgJqDediI"
   },
   "outputs": [],
   "source": [
    "plt.plot(score_arr2, label ='accumulated scores in one game play')\n",
    "plt.legend(loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
